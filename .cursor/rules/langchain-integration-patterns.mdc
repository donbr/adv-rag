---
description: 
globs: 
alwaysApply: true
---
# LangChain Integration & LCEL Patterns

## LangChain LCEL Architecture

This project leverages LangChain Expression Language (LCEL) for composable, asynchronous RAG chains. All chain construction follows LCEL patterns for streaming, parallelization, and observability.

## Core LCEL Chain Patterns

### Basic RAG Chain Construction
**[chain_factory.py](mdc:src/chain_factory.py)** implements standardized LCEL chain assembly:

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

def create_rag_chain(
    retriever: BaseRetriever,
    llm: BaseChatModel,
    prompt_template: str = None
) -> Runnable:
    """Create LCEL RAG chain with proper composition"""
    
    # Default RAG prompt
    if not prompt_template:
        prompt_template = """Answer the question based on the context below.
        
Context: {context}
Question: {question}
Answer:"""
    
    prompt = ChatPromptTemplate.from_template(prompt_template)
    
    # LCEL chain with parallel retrieval and context formatting
    rag_chain = (
        RunnableParallel({
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        })
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

def format_docs(docs: List[Document]) -> str:
    """Format retrieved documents for LLM context"""
    return "\n\n".join([
        f"Document {i+1}:\n{doc.page_content}"
        for i, doc in enumerate(docs)
    ])
```

### Advanced Chain Composition

#### Multi-Step RAG with Query Rewriting
```python
def create_multi_step_rag_chain(
    retriever: BaseRetriever,
    llm: BaseChatModel
) -> Runnable:
    """Multi-step RAG with query enhancement and verification"""
    
    # Query rewriting step
    query_rewriter = (
        ChatPromptTemplate.from_template(
            "Rewrite this question to be more specific for document search: {question}"
        )
        | llm
        | StrOutputParser()
    )
    
    # Retrieval with enhanced query
    retrieval_chain = (
        {"enhanced_query": query_rewriter, "original_question": RunnablePassthrough()}
        | RunnableParallel({
            "documents": itemgetter("enhanced_query") | retriever,
            "question": itemgetter("original_question")
        })
    )
    
    # Answer generation with source verification
    answer_chain = (
        retrieval_chain
        | RunnableParallel({
            "answer": create_answer_chain(llm),
            "sources": itemgetter("documents") | format_sources,
            "question": itemgetter("question")
        })
    )
    
    return answer_chain

def create_answer_chain(llm: BaseChatModel) -> Runnable:
    """Answer generation with citation requirements"""
    prompt = ChatPromptTemplate.from_template("""
    Answer the question based on the provided documents. 
    Include specific citations using [Doc X] format.
    
    Documents: {documents}
    Question: {question}
    
    Answer with citations:
    """)
    
    return (
        prompt
        | llm
        | StrOutputParser()
    )
```

#### Hybrid Retrieval Chain
```python
def create_hybrid_retrieval_chain(
    vector_retriever: BaseRetriever,
    bm25_retriever: BaseRetriever,
    reranker: Optional[BaseRetriever] = None
) -> Runnable:
    """Hybrid retrieval combining vector and keyword search"""
    
    # Parallel retrieval from multiple sources
    parallel_retrieval = RunnableParallel({
        "vector_docs": vector_retriever,
        "bm25_docs": bm25_retriever,
        "query": RunnablePassthrough()
    })
    
    # Document fusion and deduplication
    fusion_chain = (
        parallel_retrieval
        | RunnableLambda(fuse_and_deduplicate_docs)
    )
    
    # Optional reranking step
    if reranker:
        return fusion_chain | reranker
    
    return fusion_chain

def fuse_and_deduplicate_docs(retrieval_results: dict) -> List[Document]:
    """Fuse results from multiple retrievers with RRF scoring"""
    vector_docs = retrieval_results["vector_docs"]
    bm25_docs = retrieval_results["bm25_docs"]
    
    # Reciprocal Rank Fusion (RRF)
    doc_scores = {}
    
    # Score vector results
    for i, doc in enumerate(vector_docs):
        doc_id = doc.page_content[:100]  # Simple dedup key
        doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1 / (i + 1)
    
    # Score BM25 results
    for i, doc in enumerate(bm25_docs):
        doc_id = doc.page_content[:100]
        doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1 / (i + 1)
    
    # Return top-scored documents
    all_docs = {doc.page_content[:100]: doc for doc in vector_docs + bm25_docs}
    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
    
    return [all_docs[doc_id] for doc_id, _ in sorted_docs[:10]]
```

## Streaming and Async Patterns

### Streaming RAG Responses
```python
async def create_streaming_rag_chain(
    retriever: BaseRetriever,
    llm: BaseChatModel
) -> Runnable:
    """RAG chain with streaming support for real-time responses"""
    
    prompt = ChatPromptTemplate.from_template("""
    Answer based on the context. Stream your response naturally.
    
    Context: {context}
    Question: {question}
    Answer:
    """)
    
    # Async retrieval with context preparation
    async def retrieve_and_format(question: str) -> dict:
        docs = await retriever.aget_relevant_documents(question)
        return {
            "context": format_docs(docs),
            "question": question
        }
    
    # Streaming chain
    streaming_chain = (
        RunnableLambda(retrieve_and_format)
        | prompt
        | llm.with_config({"tags": ["streaming"]})
        | StrOutputParser()
    )
    
    return streaming_chain

# Usage in FastAPI endpoint
@app.post("/chat/stream")
async def stream_chat(request: ChatRequest):
    """Streaming chat endpoint with LCEL chain"""
    chain = await create_streaming_rag_chain(retriever, llm)
    
    async def generate():
        async for chunk in chain.astream({"question": request.question}):
            yield f"data: {json.dumps({'content': chunk})}\n\n"
    
    return StreamingResponse(generate(), media_type="text/plain")
```

### Batch Processing with LCEL
```python
async def process_batch_queries(
    queries: List[str],
    chain: Runnable,
    batch_size: int = 5
) -> List[str]:
    """Process multiple queries efficiently with batching"""
    
    # Batch processing with concurrency control
    results = []
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i + batch_size]
        
        # Parallel execution within batch
        batch_results = await chain.abatch(
            [{"question": q} for q in batch],
            config={"max_concurrency": batch_size}
        )
        
        results.extend(batch_results)
    
    return results
```

## MCP Integration with LangChain

### MCP Tool Wrapper for LangChain
```python
from langchain_mcp_adapters import create_langchain_tool

def create_mcp_langchain_integration():
    """Integrate MCP server tools with LangChain agents"""
    
    # Convert MCP tools to LangChain tools
    semantic_search_tool = create_langchain_tool(
        server_path="src/mcp_server.py",
        tool_name="semantic_search",
        description="Search for relevant documents using hybrid retrieval"
    )
    
    document_query_tool = create_langchain_tool(
        server_path="src/mcp_server.py", 
        tool_name="document_query",
        description="Query specific documents with follow-up questions"
    )
    
    return [semantic_search_tool, document_query_tool]

# Agent with MCP tools
from langchain.agents import create_openai_tools_agent, AgentExecutor

async def create_rag_agent():
    """Create LangChain agent with MCP-powered RAG tools"""
    
    # Get MCP tools
    mcp_tools = create_mcp_langchain_integration()
    
    # Agent prompt
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful research assistant with access to 
        document search and query tools. Use the tools to find relevant 
        information and provide comprehensive answers."""),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ])
    
    # Create agent
    llm = get_chat_openai()  # From src/llm_models.py
    agent = create_openai_tools_agent(llm, mcp_tools, prompt)
    
    return AgentExecutor(
        agent=agent,
        tools=mcp_tools,
        verbose=True,
        max_iterations=5
    )
```

### LangGraph Integration
```python
from langgraph import StateGraph, END
from langgraph.prebuilt import create_react_agent

def create_rag_graph():
    """Create LangGraph workflow with RAG capabilities"""
    
    class RAGState(TypedDict):
        question: str
        documents: List[Document]
        answer: str
        confidence: float
    
    # Define workflow nodes
    async def retrieve_documents(state: RAGState) -> RAGState:
        retriever = create_retriever("hybrid", vectorstore)
        docs = await retriever.aget_relevant_documents(state["question"])
        
        return {
            **state,
            "documents": docs
        }
    
    async def generate_answer(state: RAGState) -> RAGState:
        chain = create_rag_chain(retriever, llm)
        answer = await chain.ainvoke({
            "context": format_docs(state["documents"]),
            "question": state["question"]
        })
        
        # Simple confidence scoring
        confidence = min(len(state["documents"]) / 5.0, 1.0)
        
        return {
            **state,
            "answer": answer,
            "confidence": confidence
        }
    
    async def decide_next_step(state: RAGState) -> str:
        if state["confidence"] > 0.7:
            return END
        else:
            return "retrieve_more"
    
    # Build graph
    workflow = StateGraph(RAGState)
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("generate", generate_answer)
    workflow.add_conditional_edges(
        "generate",
        decide_next_step,
        {"retrieve_more": "retrieve", END: END}
    )
    
    workflow.set_entry_point("retrieve")
    
    return workflow.compile()
```

## Error Handling and Observability

### LCEL Chain Error Handling
```python
from langchain_core.runnables import RunnableWithFallbacks

def create_resilient_rag_chain(
    primary_retriever: BaseRetriever,
    fallback_retriever: BaseRetriever,
    llm: BaseChatModel
) -> Runnable:
    """RAG chain with fallback mechanisms"""
    
    # Primary chain
    primary_chain = create_rag_chain(primary_retriever, llm)
    
    # Fallback chain with simpler retrieval
    fallback_chain = create_rag_chain(fallback_retriever, llm)
    
    # Chain with fallbacks
    resilient_chain = RunnableWithFallbacks(
        runnable=primary_chain,
        fallbacks=[fallback_chain],
        exception_key="error"
    )
    
    return resilient_chain
```

### LangSmith Integration
```python
from langsmith import traceable

@traceable(name="rag_pipeline")
async def traced_rag_query(question: str) -> str:
    """RAG query with LangSmith tracing"""
    
    # Automatic tracing of LCEL chains
    chain = create_rag_chain(retriever, llm)
    
    result = await chain.ainvoke(
        {"question": question},
        config={
            "tags": ["production", "rag"],
            "metadata": {"user_id": get_current_user()}
        }
    )
    
    return result
```

## Performance Optimization

### Chain Compilation and Caching
```python
from langchain_core.runnables import RunnableConfig

# Compile chain for better performance
compiled_chain = create_rag_chain(retriever, llm).with_config(
    RunnableConfig(
        tags=["compiled"],
        metadata={"optimized": True}
    )
)

# Cache expensive operations
from functools import lru_cache

@lru_cache(maxsize=100)
def get_cached_retriever(collection_name: str) -> BaseRetriever:
    """Cache retriever instances for reuse"""
    vectorstore = setup_qdrant(collection_name)
    return create_retriever("hybrid", vectorstore)
```

These patterns ensure robust, scalable LangChain integration while maintaining compatibility with MCP protocols and production requirements.
