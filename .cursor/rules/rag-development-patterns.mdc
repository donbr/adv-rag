---
description: 
globs: 
alwaysApply: true
---
# RAG Development Patterns & Best Practices

## RAG Pipeline Architecture

This project implements a sophisticated RAG pipeline with hybrid search, LangChain LCEL chains, and production-ready patterns. All RAG components follow standardized interfaces for maintainability and testability.

## Core RAG Components

### Retrieval Factory Pattern
**[retriever_factory.py](mdc:src/retriever_factory.py)** implements the factory pattern for retrieval strategies:

```python
# Standard retrieval interface
def create_retriever(
    retrieval_type: str,
    vectorstore: VectorStore,
    search_kwargs: dict = None
) -> BaseRetriever:
    """Factory function for consistent retriever creation"""
```

**Supported Retrieval Types:**
- `similarity`: Basic vector similarity search
- `mmr`: Maximum Marginal Relevance for diversity
- `hybrid`: BM25 + vector search combination
- `contextual`: Context-aware retrieval with metadata filtering

### Chain Factory Pattern
**[chain_factory.py](mdc:src/chain_factory.py)** assembles LangChain LCEL chains with standardized interfaces:

```python
# LCEL chain construction
def create_rag_chain(
    retriever: BaseRetriever,
    llm: BaseChatModel,
    prompt_template: str = None
) -> RunnableSequence:
    """Creates production-ready RAG chain with LCEL"""
```

**Chain Features:**
- Prompt template injection
- Context formatting
- Response parsing
- Error handling
- Streaming support

## Vector Store Management

### Qdrant Configuration
**[vectorstore_setup.py](mdc:src/vectorstore_setup.py)** manages vector database lifecycle:

```python
# Production Qdrant setup
def setup_qdrant(
    collection_name: str,
    embeddings: Embeddings,
    url: str = None,
    api_key: str = None
) -> QdrantVectorStore:
    """Initialize Qdrant with proper indexing and configuration"""
```

**Best Practices:**
- Collection naming conventions: `{project}_{environment}_{version}`
- Index configuration for optimal retrieval performance
- Connection pooling for production workloads
- Backup and recovery strategies

### Embedding Strategy
**[embeddings.py](mdc:src/embeddings.py)** provides centralized embedding management:

```python
# Pinned embedding model (see model-pinning.mdc)
def get_openai_embeddings() -> OpenAIEmbeddings:
    return OpenAIEmbeddings(model="text-embedding-3-small")
```

## Document Processing Pipeline

### Data Loading Patterns
**[data_loader.py](mdc:src/data_loader.py)** implements robust document ingestion:

```python
# Standard document processing interface
def load_documents(
    source_path: str,
    loader_type: str,
    chunk_size: int = 1000,
    chunk_overlap: int = 200
) -> List[Document]:
    """Load and chunk documents with metadata preservation"""
```

**Supported Document Types:**
- PDF files (with layout preservation)
- Text files and Markdown
- Web scraping (with rate limiting)
- Structured data (JSON, CSV)

**Chunking Strategies:**
- Semantic chunking for better retrieval
- Overlapping windows for context preservation
- Metadata enrichment for filtering
- Quality validation and error handling

## Testing Patterns

### RAG Pipeline Testing
```python
# Test retrieval quality
def test_retrieval_relevance():
    retriever = create_retriever("hybrid", vectorstore)
    results = retriever.get_relevant_documents("test query")
    assert len(results) > 0
    assert all(result.metadata for result in results)

# Test chain end-to-end
def test_rag_chain_response():
    chain = create_rag_chain(retriever, llm)
    response = chain.invoke({"question": "test question"})
    assert "answer" in response
    assert len(response["answer"]) > 0
```

### Integration Testing
- **Retrieval Quality**: Measure precision@k and recall@k
- **Response Quality**: LLM-as-a-judge evaluation
- **Latency Testing**: P95 response times under load
- **Data Pipeline**: End-to-end document ingestion validation

## Performance Optimization

### Retrieval Optimization
```python
# Hybrid search with score normalization
def hybrid_search(
    query: str,
    vector_weight: float = 0.7,
    bm25_weight: float = 0.3,
    top_k: int = 10
) -> List[Document]:
    """Optimized hybrid search with configurable weights"""
```

### Caching Strategies
- **Embedding Cache**: LRU cache for repeated queries
- **Retrieval Cache**: Redis-based result caching
- **Chain Cache**: LangChain built-in caching

### Monitoring and Observability
**[logging_config.py](mdc:src/logging_config.py)** provides structured logging:

```python
# OpenTelemetry integration
@trace_function("rag_retrieval")
def retrieve_documents(query: str) -> List[Document]:
    # Automatic tracing for performance monitoring
```

**Key Metrics:**
- Retrieval latency and throughput
- LLM token usage and costs
- Cache hit rates
- Error rates by component

## Production Considerations

### Scalability Patterns
- **Horizontal Scaling**: Stateless retrieval services
- **Vector Store Sharding**: Collection-based partitioning
- **Load Balancing**: Round-robin with health checks
- **Rate Limiting**: Per-user and per-endpoint limits

### Error Handling
```python
# Graceful degradation pattern
async def rag_with_fallback(query: str) -> str:
    try:
        return await full_rag_chain.ainvoke({"question": query})
    except VectorStoreException:
        return await fallback_chain.ainvoke({"question": query})
    except Exception as e:
        logger.error(f"RAG pipeline error: {e}")
        return "I apologize, but I'm unable to process your request right now."
```

### Security Considerations
- Input validation and sanitization
- Output filtering for sensitive information
- API rate limiting and authentication
- Vector store access controls

## Integration with MCP

### MCP Tool Patterns
When exposing RAG functionality via MCP tools:

```python
@mcp.tool()
async def semantic_search(query: str, top_k: int = 5) -> str:
    """Expose retrieval as MCP tool for LLM agents"""
    retriever = create_retriever("hybrid", vectorstore)
    docs = retriever.get_relevant_documents(query)
    return format_documents_for_llm(docs[:top_k])
```

### Resource Patterns
```python
@mcp.resource("rag://collections/{collection_name}")
async def get_collection_info(collection_name: str) -> str:
    """Expose vector store metadata as MCP resource"""
    info = await get_collection_stats(collection_name)
    return json.dumps(info, indent=2)
```

Follow these patterns to maintain consistency across the RAG pipeline and ensure production readiness.
